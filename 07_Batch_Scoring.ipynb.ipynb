{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64dd4a49-0acc-4b8c-810d-e906a2b379c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "ALTER TABLE fraude_qr.gold.alerts_scored ADD COLUMNS (expected_loss DOUBLE, created_at TIMESTAMP);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dde9426-09eb-41f9-a3c9-f9d83d191d4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCE6 Cargando modelo: models:/fraude_qr.ml.detection_model_v1/1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/databricks/sdk/errors/base.py:87: UserWarning: The 'retry_after_secs' parameter of DatabricksError is deprecated and will be removed in a future version.\n  warnings.warn(\n2025/09/20 04:43:37 WARNING mlflow.pyfunc: Calling `spark_udf()` with `env_manager=\"local\"` does not recreate the same environment that was used during training, which may lead to errors or inaccurate predictions. We recommend specifying `env_manager=\"conda\"`, which automatically recreates the environment that was used to train the model and performs inference in the recreated environment.\n2025/09/20 04:43:37 INFO mlflow.models.flavor_backend_registry: Selected backend for flavor 'python_function'\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDCD6 Cargando nuevos datos desde: fraude_qr.silver.qr_transactions\n\uD83D\uDEE0️ Aplicando la misma lógica de feature engineering...\n\uD83D\uDD2E Generando scores de fraude...\n\uD83D\uDCBE Escribiendo alertas en: fraude_qr.gold.alerts_scored\n\uD83C\uDF89 ¡Proceso de scoring batch completado!\n\n\uD83D\uDD0D Muestra de alertas generadas:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>tx_id</th><th>scored_at</th><th>scored_date</th><th>score</th><th>triage_policy</th><th>model_version</th><th>expected_loss</th><th>created_at</th></tr></thead><tbody><tr><td>1304959</td><td>2025-09-20T04:43:38.504Z</td><td>2025-09-20</td><td>0.0</td><td>OK</td><td>v1</td><td>0.0</td><td>null</td></tr><tr><td>1691262</td><td>2025-09-20T04:43:38.504Z</td><td>2025-09-20</td><td>0.0</td><td>OK</td><td>v1</td><td>0.0</td><td>null</td></tr><tr><td>1326491</td><td>2025-09-20T04:43:38.504Z</td><td>2025-09-20</td><td>0.0</td><td>OK</td><td>v1</td><td>0.0</td><td>null</td></tr><tr><td>1774315</td><td>2025-09-20T04:43:38.504Z</td><td>2025-09-20</td><td>0.0</td><td>OK</td><td>v1</td><td>0.0</td><td>null</td></tr><tr><td>1906521</td><td>2025-09-20T04:43:38.504Z</td><td>2025-09-20</td><td>0.0</td><td>OK</td><td>v1</td><td>0.0</td><td>null</td></tr><tr><td>1198234</td><td>2025-09-20T04:43:38.504Z</td><td>2025-09-20</td><td>0.0</td><td>OK</td><td>v1</td><td>0.0</td><td>null</td></tr><tr><td>1222511</td><td>2025-09-20T04:43:38.504Z</td><td>2025-09-20</td><td>0.0</td><td>OK</td><td>v1</td><td>0.0</td><td>null</td></tr><tr><td>1494491</td><td>2025-09-20T04:43:38.504Z</td><td>2025-09-20</td><td>0.0</td><td>OK</td><td>v1</td><td>0.0</td><td>null</td></tr><tr><td>1609699</td><td>2025-09-20T04:43:38.504Z</td><td>2025-09-20</td><td>0.0</td><td>OK</td><td>v1</td><td>0.0</td><td>null</td></tr><tr><td>1571183</td><td>2025-09-20T04:43:38.504Z</td><td>2025-09-20</td><td>0.0</td><td>OK</td><td>v1</td><td>0.0</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1304959,
         "2025-09-20T04:43:38.504Z",
         "2025-09-20",
         0.0,
         "OK",
         "v1",
         0.0,
         null
        ],
        [
         1691262,
         "2025-09-20T04:43:38.504Z",
         "2025-09-20",
         0.0,
         "OK",
         "v1",
         0.0,
         null
        ],
        [
         1326491,
         "2025-09-20T04:43:38.504Z",
         "2025-09-20",
         0.0,
         "OK",
         "v1",
         0.0,
         null
        ],
        [
         1774315,
         "2025-09-20T04:43:38.504Z",
         "2025-09-20",
         0.0,
         "OK",
         "v1",
         0.0,
         null
        ],
        [
         1906521,
         "2025-09-20T04:43:38.504Z",
         "2025-09-20",
         0.0,
         "OK",
         "v1",
         0.0,
         null
        ],
        [
         1198234,
         "2025-09-20T04:43:38.504Z",
         "2025-09-20",
         0.0,
         "OK",
         "v1",
         0.0,
         null
        ],
        [
         1222511,
         "2025-09-20T04:43:38.504Z",
         "2025-09-20",
         0.0,
         "OK",
         "v1",
         0.0,
         null
        ],
        [
         1494491,
         "2025-09-20T04:43:38.504Z",
         "2025-09-20",
         0.0,
         "OK",
         "v1",
         0.0,
         null
        ],
        [
         1609699,
         "2025-09-20T04:43:38.504Z",
         "2025-09-20",
         0.0,
         "OK",
         "v1",
         0.0,
         null
        ],
        [
         1571183,
         "2025-09-20T04:43:38.504Z",
         "2025-09-20",
         0.0,
         "OK",
         "v1",
         0.0,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "tx_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "scored_at",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{\"comment\": \"Fecha derivada de scored_at para particionamiento\"}",
         "name": "scored_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "score",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "triage_policy",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "model_version",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "expected_loss",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "created_at",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MAGIC %md\n",
    "# MAGIC # ⚙️ 06_Batch_Scoring\n",
    "# MAGIC Carga el modelo de producción desde el Model Registry y lo usa para calificar nuevas transacciones.\n",
    "# MAGIC - Carga la última versión del modelo `fraude-qr.ml.detection_model_v1`.\n",
    "# MAGIC - Lee nuevas transacciones de la capa Silver que necesitan ser calificadas.\n",
    "# MAGIC - Aplica la misma lógica de ingeniería de features que en el entrenamiento.\n",
    "# MAGIC - Genera scores de fraude y una política de triage.\n",
    "# MAGIC - Escribe los resultados en `fraude_qr.gold.alerts_scored`.\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import mlflow\n",
    "from pyspark.sql.functions import col, lit, struct\n",
    "\n",
    "# --- 1. Configuración ---\n",
    "# Nombre del modelo en Unity Catalog\n",
    "model_name = \"fraude_qr.ml.detection_model_v1\"\n",
    "# Usaremos la última versión disponible del modelo\n",
    "model_version = \"1\" \n",
    "model_uri = f\"models:/{model_name}/{model_version}\"\n",
    "\n",
    "# Tabla de origen (nuevos datos a calificar)\n",
    "source_table = \"fraude_qr.silver.qr_transactions\"\n",
    "# Tabla de destino para las alertas\n",
    "scored_table = \"fraude_qr.gold.alerts_scored\"\n",
    "\n",
    "print(f\"\uD83D\uDCE6 Cargando modelo: {model_uri}\")\n",
    "\n",
    "# --- 2. Cargar el Modelo ---\n",
    "# MLflow carga el modelo como una UDF de PySpark, lo que facilita su aplicación a gran escala.\n",
    "logged_model = mlflow.pyfunc.spark_udf(spark, model_uri=model_uri, result_type='double')\n",
    "\n",
    "# --- 3. Cargar Nuevos Datos ---\n",
    "# En un escenario real, aquí filtrarías por transacciones no calificadas.\n",
    "# Por simplicidad, calificaremos todo el dataset de Silver.\n",
    "print(f\"\uD83D\uDCD6 Cargando nuevos datos desde: {source_table}\")\n",
    "df_to_score = spark.table(source_table)\n",
    "\n",
    "# --- 4. Aplicar Ingeniería de Features ---\n",
    "# ⚠️ IMPORTANTE: Debes aplicar EXACTAMENTE la misma lógica de features que en el entrenamiento.\n",
    "# Por ahora, simularemos que las features ya están en la tabla Silver.\n",
    "# En un proyecto real, refactorizarías la lógica de features en una función compartida.\n",
    "print(\"\uD83D\uDEE0️ Aplicando la misma lógica de feature engineering...\")\n",
    "# (Aquí iría la misma lógica de ventanas, UDF de haversine, etc., del notebook 04)\n",
    "# Por simplicidad, asumimos que las columnas necesarias existen y creamos placeholders.\n",
    "# ESTA ES UNA SIMPLIFICACIÓN PARA ESTE EJEMPLO.\n",
    "df_with_features = (\n",
    "    df_to_score\n",
    "    .withColumn(\"distance_km\", lit(10.5))\n",
    "    .withColumn(\"payer_tx_count_1h\", lit(2))\n",
    "    .withColumn(\"payer_tx_count_24h\", lit(5))\n",
    "    .withColumn(\"amount_zscore_payer_7d\", lit(1.2))\n",
    ")\n",
    "features = [\n",
    "    \"amount\", \"distance_km\", \"payer_tx_count_1h\",\n",
    "    \"payer_tx_count_24h\", \"amount_zscore_payer_7d\", \"mcc\"\n",
    "]\n",
    "\n",
    "# --- 5. Generar Scores ---\n",
    "print(\"\uD83D\uDD2E Generando scores de fraude...\")\n",
    "df_scored = (\n",
    "    df_with_features\n",
    "    # Aplicamos el modelo (UDF) a una struct de las columnas de features.\n",
    "    .withColumn(\"score\", logged_model(struct(*map(col, features))))\n",
    ")\n",
    "\n",
    "# --- 6. Aplicar Política de Triage ---\n",
    "# Aquí puedes aplicar umbrales para clasificar las alertas.\n",
    "from pyspark.sql.functions import when\n",
    "df_alertas = (\n",
    "    df_scored\n",
    "    .withColumn(\"triage_policy\", \n",
    "        when(col(\"score\") >= 0.85, \"ALTO_RIESGO\")\n",
    "        .when(col(\"score\") >= 0.5, \"REVISAR\")\n",
    "        .otherwise(\"OK\")\n",
    "    )\n",
    "    .withColumn(\"expected_loss\", col(\"score\") * col(\"amount\"))\n",
    "    .withColumn(\"model_version\", lit(f\"v{model_version}\"))\n",
    ")\n",
    "\n",
    "# --- 7. Escribir en la Tabla Gold (Versión Corregida) ---\n",
    "from pyspark.sql.functions import to_date, current_timestamp\n",
    "\n",
    "# Preparamos el DataFrame final con el esquema correcto\n",
    "df_final_alerts = (\n",
    "    df_alertas\n",
    "    # CORRECCIÓN 1: Renombramos 'created_at' a 'scored_at' y creamos la columna de partición.\n",
    "    .withColumn(\"scored_at\", current_timestamp())\n",
    "    .withColumn(\"scored_date\", to_date(col(\"scored_at\")))\n",
    "    # CORRECCIÓN 2: Seleccionamos las columnas en el orden y con los nombres correctos\n",
    "    .select(\n",
    "        col(\"tx_id\").cast(\"long\"), # Aseguramos el tipo\n",
    "        col(\"scored_at\"),\n",
    "        col(\"scored_date\"),\n",
    "        col(\"score\"),\n",
    "        col(\"triage_policy\"),\n",
    "        col(\"model_version\"),\n",
    "        col(\"expected_loss\") # Dejamos la columna nueva para que mergeSchema la añada\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"\uD83D\uDCBE Escribiendo alertas en: {scored_table}\")\n",
    "(\n",
    "    df_final_alerts.write\n",
    "    .mode(\"overwrite\")\n",
    "    # CORRECCIÓN 3: Añadimos la opción 'mergeSchema' para permitir añadir 'expected_loss'\n",
    "    .option(\"mergeSchema\", \"true\") \n",
    "    .saveAsTable(scored_table)\n",
    ")\n",
    "\n",
    "print(\"\uD83C\uDF89 ¡Proceso de scoring batch completado!\")\n",
    "\n",
    "# --- 8. Verificación ---\n",
    "print(\"\\n\uD83D\uDD0D Muestra de alertas generadas:\")\n",
    "spark.table(scored_table).orderBy(col(\"score\").desc()).limit(10).display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8341375209016334,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "07_Batch_Scoring.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}